{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP61U4t3N/WxM09tF2If2no",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshM01/Chatbot-using-RNN/blob/main/220591_Lokesh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
      ],
      "metadata": {
        "id": "0Z8xnofSXun3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_babi_task(task_number):\n",
        "  train_file = f\"train_qa.txt\"\n",
        "  test_file = f\"test_qa.txt\"\n",
        "  def read_data(file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        return [line.strip().lower() for line in lines]\n",
        "\n",
        "  def process_data(lines):\n",
        "        data = []\n",
        "        context = []\n",
        "        for line in lines:\n",
        "            if line.startswith(\"1 \"):\n",
        "                context = []\n",
        "            if \"\\t\" in line:\n",
        "                q, a, _ = map(str.lower, line.split(\"\\t\"))\n",
        "                data.append((context.copy(), q, a))\n",
        "            else:\n",
        "                context.append(line)\n",
        "        return data\n",
        "\n",
        "  train_data = process_data(read_data(train_file))\n",
        "  test_data = process_data(read_data(test_file))\n",
        "\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "rYNdWWguVyJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, word2idx):\n",
        "    context_data = []\n",
        "    question_data = []\n",
        "    answer_data = []\n",
        "\n",
        "    for context, question, answer in data:\n",
        "        context_ids = [word2idx[word] for word in ' '.join(context).split()]\n",
        "        question_ids = [word2idx[word] for word in question.split()]\n",
        "        answer_ids = [word2idx[word] for word in answer.split()]\n",
        "\n",
        "        context_data.append(context_ids)\n",
        "        question_data.append(question_ids)\n",
        "        answer_data.append(answer_ids)\n",
        "\n",
        "    return (\n",
        "        pad_sequences(context_data, padding='post'),\n",
        "        pad_sequences(question_data, padding='post'),\n",
        "        pad_sequences(answer_data, padding='post')\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3uDt7xUXQ1fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = load_babi_task(1)"
      ],
      "metadata": {
        "id": "680MljtYQdnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "for context, question, answer in train_data + test_data:\n",
        "    vocab.update(set(' '.join(context).split()))\n",
        "    vocab.update(set(question.split()))\n",
        "    vocab.update(set(answer.split()))\n",
        "\n",
        "word2idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx + 1: word for idx, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "MsQAyAQGQyKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_context, train_question, train_answer = preprocess_data(train_data, word2idx)\n",
        "test_context, test_question, test_answer = preprocess_data(test_data, word2idx)"
      ],
      "metadata": {
        "id": "s0fagmGSRF3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "embedding_dim = 50\n",
        "hidden_units = 50"
      ],
      "metadata": {
        "id": "POxwuc6nRK1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=train_context.shape[1]))\n",
        "model.add(SimpleRNN(hidden_units, return_sequences=True))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "metadata": {
        "id": "SX--wkJ-RNS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QljbIjXPRPYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "model.fit(train_context, train_answer, epochs=num_epochs, batch_size=batch_size, validation_split=0.1)"
      ],
      "metadata": {
        "id": "k-ArOQB_RSAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}